{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC2611 Exercise: Meaning construction from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import NLTK in Python: http://www.nltk.org/. Download the Brown Corpus http://www.nltk.org/book/ch02.html for analyses below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Jungeun (June)\n",
      "[nltk_data]     Lim\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "nltk.download('brown')\n",
    "\n",
    "# read in lowercased words\n",
    "brown_lower_words = [x.lower() for x in brown.words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2-1. Extract the 5000 most common English words (denoted by W) based on unigram frequencies in the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "K=5000\n",
    "\n",
    "def create_vocab(data):\n",
    "    \"\"\"Extract the K most common words based on unigram frequencies in the given corpus.\n",
    "    Return the list of the K words.\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab=Counter()\n",
    "    for i, word in enumerate(data):\n",
    "        # considering multiple words connected with punctuations such as\n",
    "        # \"face-saving\", \"a.k.a.\", or \"you're\" as well\n",
    "        if re.match('^[a-z]', word):\n",
    "            vocab[word]+=1\n",
    "        \n",
    "    W=[word for word,v in vocab.most_common(K)]\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report the 5 most and least common words you have found in the 5000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 most common words: ['the', 'of', 'and', 'to', 'a']\n",
      "The 5 least common words: ['packed', 'lacked', 'condemned', 'documents', 'corporate']\n"
     ]
    }
   ],
   "source": [
    "W = create_vocab(brown_lower_words)\n",
    "\n",
    "print(\"The 5 most common words:\", W[:5])\n",
    "print(\"The 5 least common words:\", W[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2-2. Update W by adding n words where n is the set of words in Table 1 of RG65 that were not included in the top 5000 words from the Brown corpus. Denote the total number of words in W as |W|."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding the words from RG65, the total number of words in W is now: 5030\n"
     ]
    }
   ],
   "source": [
    "rg65_fname=\"data/RG65_table1.txt\"\n",
    "\n",
    "more_words=set([w for w in re.split(r'\\s|\\n', open(rg65_fname).read()) if w.isalpha()])\n",
    "\n",
    "for w in more_words:\n",
    "    if not w in W:\n",
    "        W.append(w)\n",
    "        \n",
    "print(\"After adding the words from RG65, the total number of words in W is now:\", len(W))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Construct a word-context vector model (denoted by M1) by collecting bigram counts for words in W. The output should be a |W|*|W| matrix (consider using sparse matrices for better efficiency), where each row is a word in W, and each column is a context in W that precedes row words in sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def make_word_context_vector(data, W):\n",
    "    M1 = pd.DataFrame(0, columns=W, index=W)\n",
    "    \n",
    "    for i, word in enumerate(data):\n",
    "        if type(word) != str or word not in W or i == 0:\n",
    "            continue\n",
    "        context = data[i-1]\n",
    "        if context in W:\n",
    "            M1[context][word]+=1\n",
    "        \n",
    "    return M1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# takes 2-3 mins\n",
    "\n",
    "M1 = make_word_context_vector(brown_lower_words, W)\n",
    "M1 = M1.astype(pd.SparseDtype(\"int\", 0))\n",
    "\n",
    "#save the matrix for future use\n",
    "#M1.to_pickle(\"./M1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the saved matrix as a .pkl file may be loaded instead of building it from scratch\n",
    "#M1 = pd.read_pickle(\"./M1.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Compute positive pointwise mutual information on M1. Denote this model as M1+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppmi(M1):\n",
    "    \"\"\"Given a 2d pandas dataframe filled with raw word co-occurrence counts,\n",
    "    Return a 2d pandas dataframe filled with positive pointwise mutual information score for each entry.\n",
    "    Formula: log( p(x|y)/p(x) ), y being the column(context-word), x being the row(word).\n",
    "    PMI is zero if X and Y are independent. \n",
    "    PPMI considers negative PMI as zero.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get numpy array from pandas dataframe for faster computation\n",
    "    arr = M1.to_numpy()\n",
    "\n",
    "    # p(x|y)\n",
    "    col_totals = arr.sum(axis=0).astype(int)\n",
    "    prob_rows_given_col = arr / col_totals\n",
    "\n",
    "    # p(x)\n",
    "    row_totals = arr.sum(axis=1).astype(int)\n",
    "    prob_rows = row_totals / sum(row_totals)\n",
    "\n",
    "    # PMI: log( p(x|y) / p(x) )\n",
    "    ratio = (prob_rows_given_col.T / prob_rows).T\n",
    "    ratio[ratio==0] = 0.00001\n",
    "    pmi = np.log(ratio)\n",
    "    pmi[pmi < 0] = 0\n",
    "\n",
    "    return pd.DataFrame(data=pmi, index=M1.index, columns=M1.columns).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jungeun (June) Lim\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n",
      "C:\\Users\\Jungeun (June) Lim\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in true_divide\n",
      "C:\\Users\\Jungeun (June) Lim\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "M1_plus = ppmi(M1)\n",
    "M1_plus = M1_plus.astype(pd.SparseDtype(\"int\", 0))\n",
    "\n",
    "#save the matrix for future use\n",
    "#M1_plus.to_pickle(\"./M1_plus.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the saved matrix as a .pkl file may be loaded instead of building it from scratch\n",
    "#M1_plus = pd.read_pickle(\"./M1_plus.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Construct a latent semantic model (denoted by M2) by applying principal components analysis to M1+. The output should return 3 matrices, with different truncated dimenions at 10 (or a |W|*10 matrix, denoted by M2_10), 100 (M2_100), and 300 (M2_300)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "def pca(df, n):\n",
    "    arr = PCA(n_components=n).fit_transform(M1_plus)\n",
    "    return pd.DataFrame(arr, columns=np.arange(n), index=M1_plus.index)\n",
    "\n",
    "M2_10 = pca(M1_plus, 10)\n",
    "M2_100 = pca(M1_plus, 100)\n",
    "M2_300 = pca(M1_plus, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6. Find all pairs of words in Table 1 of RG65 that are also available in W. Denote these pairs as P. Record the human-judged similarities of these word pairs from the table and denote similarity values as S."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg_PS_dic = {} # this dictionary contains P as keys and S as values\n",
    "\n",
    "with open(rg65_fname, 'r',) as rg:\n",
    "    rg_list = [s.strip().split(' ') for s in rg.read().split('\\n')]\n",
    "    for i, v in enumerate(rg_list):\n",
    "        if i % 2 == 0:\n",
    "            rg_pair = tuple(v)\n",
    "        else:\n",
    "            rg_PS_dic[rg_pair] = float(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7. Perform the following calculations on each of these models M1, M1+, M2_10, M2_100, M2_300, separately: Calculate cosine similarity between each pair of words in P, based on the constructed word vectors. Record model-predicted similarities: S_M1, S_M2_10 , S_M2_100 , S_M2_300 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def cos_sim_two_words(df, w1, w2):\n",
    "    return cosine_similarity(X=df.loc[w1].to_numpy().reshape(1, -1), Y=df.loc[w2].to_numpy().reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Calculate cosine similarity between each pair of words\n",
    "# Takes about 17 seconds\n",
    "\n",
    "S_M1 = dict.fromkeys(rg_PS_dic.keys(),-1.0)\n",
    "S_M1_plus = dict.fromkeys(rg_PS_dic.keys(),-1.0)\n",
    "S_M2_10 = dict.fromkeys(rg_PS_dic.keys(),-1.0)\n",
    "S_M2_100 = dict.fromkeys(rg_PS_dic.keys(),-1.0)\n",
    "S_M2_300 = dict.fromkeys(rg_PS_dic.keys(),-1.0)\n",
    "\n",
    "for pair in rg_PS_dic:\n",
    "    S_M1[pair] = cos_sim_two_words(M1, pair[0], pair[1])[0][0]\n",
    "    S_M1_plus[pair] = cos_sim_two_words(M1_plus, pair[0], pair[1])[0][0]\n",
    "    S_M2_10[pair] = cos_sim_two_words(M2_10, pair[0], pair[1])[0][0]\n",
    "    S_M2_100[pair] = cos_sim_two_words(M2_100, pair[0], pair[1])[0][0]\n",
    "    S_M2_300[pair] = cos_sim_two_words(M2_300, pair[0], pair[1])[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8. Report Pearson correlation between S and each of the model-predicted similarities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson’s correlation coefficient between humans and M1:\t 0.025 \tp-value: 0.84\n",
      "Pearson’s correlation coefficient between humans and M1_plus:\t 0.29 \tp-value: 0.018\n",
      "Pearson’s correlation coefficient between humans and M2_10:\t 0.21 \tp-value: 0.09\n",
      "Pearson’s correlation coefficient between humans and M2_100:\t 0.33 \tp-value: 0.0068\n",
      "Pearson’s correlation coefficient between humans and M2_300:\t 0.36 \tp-value: 0.0037\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "corr_S_M1 = pearsonr(list(S_M1.values()), list(rg_PS_dic.values()))\n",
    "corr_S_M1_plus = pearsonr(list(S_M1_plus.values()), list(rg_PS_dic.values()))\n",
    "corr_S_M2_10 = pearsonr(list(S_M2_10.values()), list(rg_PS_dic.values()))\n",
    "corr_S_M2_100 = pearsonr(list(S_M2_100.values()), list(rg_PS_dic.values()))\n",
    "corr_S_M2_300 = pearsonr(list(S_M2_300.values()), list(rg_PS_dic.values()))\n",
    "\n",
    "print(f\"Pearson’s correlation coefficient between humans and M1:\\t {corr_S_M1[0]:.2}\", f\"\\tp-value: {corr_S_M1[1]:.2}\")\n",
    "print(f\"Pearson’s correlation coefficient between humans and M1_plus:\\t {corr_S_M1_plus[0]:.2}\", f\"\\tp-value: {corr_S_M1_plus[1]:.2}\")\n",
    "print(f\"Pearson’s correlation coefficient between humans and M2_10:\\t {corr_S_M2_10[0]:.2}\", f\"\\tp-value: {corr_S_M2_10[1]:.2}\")\n",
    "print(f\"Pearson’s correlation coefficient between humans and M2_100:\\t {corr_S_M2_100[0]:.2}\", f\"\\tp-value: {corr_S_M2_100[1]:.2}\")\n",
    "print(f\"Pearson’s correlation coefficient between humans and M2_300:\\t {corr_S_M2_300[0]:.2}\", f\"\\tp-value: {corr_S_M2_300[1]:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSC2611 Lab: Word embedding and semantic change\n",
    "## Part 1 Synchronic word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Download the pre-trained word2vec embeddings from https://code.google.com/archive/p/word2vec/, specifically, the file \"GoogleNews-vectors-negative300.bin.gz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# load the pre-trained word2vec embeddings (the file is manually downloaded from the provided URL)\n",
    "# takes 2-4 mins \n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 & 3. Using gensim, extract embeddings of words in Table 1 of RG65 that also appeared in the set W from the earlier exercise, i.e., the pairs of words should be identical in all analyses. Calculate cosine distance between each pair of word embeddings you have extracted, and report the Pearson correlation between word2vec-based and human similarities. Comment on this value in comparison to those from LSA and word-context vectors from analyses in the earlier exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings of words in Table 1 of RG65 that also appeared in the set W from the earlier exercise,\n",
    "# and calculate cosine distance between each pair of the word embeddings\n",
    "\n",
    "S_WB = dict.fromkeys(rg_PS_dic.keys(),-1.0)\n",
    "\n",
    "for pair in rg_PS_dic:\n",
    "    S_WB[pair] = cosine_similarity(X=model[pair[0]].reshape(1, -1), Y=model[pair[1]].reshape(1, -1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson’s correlation coefficient between humans and word2vec:\t 0.77 \tp-value: 5.1e-14\n"
     ]
    }
   ],
   "source": [
    "# Compute and report the Pearson correlation between word2vec-based and human similarities\n",
    "\n",
    "corr_S_WB = pearsonr(list(S_WB.values()), list(rg_PS_dic.values()))\n",
    "\n",
    "print(f\"Pearson’s correlation coefficient between humans and word2vec:\\t {corr_S_WB[0]:.2}\", f\"\\tp-value: {corr_S_WB[1]:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Perform the analogy test based on the provided data with the pre-trained word2vec embeddings. Report the accuracy on the semantic analogy test and the syntactic analogy test. Repeat the analysis with LSA vectors (300 dimensions) from the earlier exercise, and commment on the results in comparison to those from word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the analogy test file\n",
    "\n",
    "import requests\n",
    "\n",
    "anal_file = requests.get('http://www.fit.vutbr.cz/~imikolov/rnnlm/word-test.v1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the analogy test file\n",
    "\n",
    "anal_data = {}     \n",
    "for line in anal_file.text.split('\\n')[1:]:\n",
    "    if line.startswith(':'):\n",
    "        anal_type = line[1:].strip()\n",
    "        anal_data[anal_type] = []\n",
    "    else:\n",
    "        anal_data[anal_type].append(line.lower().strip().split())\n",
    "\n",
    "anal_data['gram9-plural-verbs'].pop() # to remove a blank line at the end of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the words that are not in the 5,000 most frequent words from the Brown corpus;\n",
    "# only the analogy test cases in which all the 4 words are in the 5,000 most frequent words will be used\n",
    "\n",
    "anal_data_final = {key: list() for key in anal_data.keys()}\n",
    "\n",
    "for anal_type, test_case_list in anal_data.items():\n",
    "    for test_case in test_case_list:\n",
    "        if (len([word for word in test_case if word in W])) == 4:\n",
    "            anal_data_final[anal_type].append(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital-common-countries: 20 cases left\n",
      "capital-world: 6 cases left\n",
      "currency: 0 cases left\n",
      "city-in-state: 46 cases left\n",
      "family: 90 cases left\n",
      "gram1-adjective-to-adverb: 380 cases left\n",
      "gram2-opposite: 20 cases left\n",
      "gram3-comparative: 240 cases left\n",
      "gram4-superlative: 42 cases left\n",
      "gram5-present-participle: 272 cases left\n",
      "gram6-nationality-adjective: 53 cases left\n",
      "gram7-past-tense: 600 cases left\n",
      "gram8-plural: 306 cases left\n",
      "gram9-plural-verbs: 132 cases left\n",
      "\n",
      "162 semantic cases to test\n",
      "2045 syntactic cases to test\n",
      "2207 cases in total to test\n"
     ]
    }
   ],
   "source": [
    "# How many analogy test cases do I have now?\n",
    "\n",
    "num_test_cases = 0\n",
    "\n",
    "for k in anal_data_final.keys():\n",
    "    num_test_cases += len(anal_data_final[k])\n",
    "    print(f\"{k}: {len(anal_data_final[k])} cases left\")\n",
    "\n",
    "num_sem_test_cases = len(anal_data_final['capital-common-countries']) + len(anal_data_final['capital-world']) + len(anal_data_final['city-in-state']) + len(anal_data_final['family'])\n",
    "num_syn_test_cases = num_test_cases - num_sem_test_cases\n",
    "print(f\"\\n{num_sem_test_cases} semantic cases to test\")\n",
    "print(f\"{num_syn_test_cases} syntactic cases to test\")\n",
    "print(f\"{num_test_cases} cases in total to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since there is no test cases of currency, remove the key from the data\n",
    "del anal_data_final['currency']\n",
    "\n",
    "# Store the analogy cases to test into text files (in the same format as the initial data file)\n",
    "# One file for semantic analogies, the other for syntatic analogies\n",
    "\n",
    "sem_file = open(\"data/semantic_analogies.txt\", 'w')\n",
    "syn_file = open(\"data/syntactic_analogies.txt\", 'w')\n",
    "\n",
    "count = 0\n",
    "for k, v in anal_data_final.items():\n",
    "    category = \": \" + k + \"\\n\"\n",
    "    count += 1\n",
    "    if count < 6:\n",
    "        FILE = sem_file\n",
    "    else:\n",
    "        FILE = syn_file\n",
    "    FILE.write(category)\n",
    "    for case in v:\n",
    "        for word in case:  \n",
    "            FILE.write(word + \" \")\n",
    "        FILE.write(\"\\n\")\n",
    "\n",
    "sem_file.close()\n",
    "syn_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Word2vec word analogy test: uses \n",
    "# takes about 1-2 minutes\n",
    "\n",
    "sem_result = model.evaluate_word_analogies(\"data/semantic_analogies.txt\")\n",
    "syn_result = model.evaluate_word_analogies(\"data/syntactic_analogies.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# LSA vectors(300 dimensions) semantic analogy test\n",
    "# takes about 2-3 minutes\n",
    "\n",
    "M2_300_np = M2_300.to_numpy()\n",
    "\n",
    "sem_correct_count = 0   # how many answers are correct?\n",
    "sem_correct_list = []   # store the correct answers\n",
    "sem_incorrect_list = [] # store the incorrect answers\n",
    "\n",
    "keys_list = list(anal_data_final.keys())\n",
    "\n",
    "for i in range(0,4):\n",
    "    for question in anal_data_final[keys_list[i]]:\n",
    "        w1 = question[0]\n",
    "        w2 = question[1]\n",
    "        w3 = question[2]\n",
    "        w4 = question[3] # word to guess!\n",
    "        \n",
    "        v1 = M2_300.loc[w1].to_numpy()\n",
    "        v2 = M2_300.loc[w2].to_numpy()\n",
    "        v3 = M2_300.loc[w3].to_numpy()\n",
    "        \n",
    "        v4_expected = v3-v1+v2\n",
    "            \n",
    "        cos_sim_max = -1\n",
    "        v4_loc = 0\n",
    "            \n",
    "        # find the closest word vector (answer) to the acquired vector in terms of cosine similarity\n",
    "        for i in range(len(M2_300_np)):\n",
    "            # see the similarity b/w each word and the expected v4\n",
    "            cos_sim = cosine_similarity(X=M2_300_np[i].reshape(1, -1), Y=v4_expected.reshape(1, -1))\n",
    "            \n",
    "            # if the word is more similar to the expected v4\n",
    "            # than any other previously seen words, record that word\n",
    "            # * the words in the analogy question is ignored\n",
    "            if cos_sim_max < cos_sim and M2_300.index[i] != w1 and M2_300.index[i] != w2 and M2_300.index[i] != w3:\n",
    "                cos_sim_max = cos_sim\n",
    "                v4_loc = i\n",
    "\n",
    "        answer = M2_300.index[v4_loc]\n",
    "        \n",
    "        if answer == w4:\n",
    "            sem_correct_count += 1\n",
    "            sem_correct_list.append([question, answer])\n",
    "        else:\n",
    "            sem_incorrect_list.append([question, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 41min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# LSA vectors(300 dimensions) syntactic analogy test\n",
    "# takes about 40 minutes!\n",
    "# I may want to find a more efficient way to find the closest vector to the given vector...\n",
    "\n",
    "syn_correct_count = 0   # how many answers are correct?\n",
    "syn_correct_list = []   # store the correct answers\n",
    "syn_incorrect_list = [] # store the incorrect answers\n",
    "\n",
    "for i in range(4,13):\n",
    "    for question in anal_data_final[keys_list[i]]:\n",
    "        w1 = question[0]\n",
    "        w2 = question[1]\n",
    "        w3 = question[2]\n",
    "        w4 = question[3] # word to guess!\n",
    "        \n",
    "        v1 = M2_300.loc[w1].to_numpy()\n",
    "        v2 = M2_300.loc[w2].to_numpy()\n",
    "        v3 = M2_300.loc[w3].to_numpy()\n",
    "        \n",
    "        v4_expected = v3-v1+v2\n",
    "            \n",
    "        cos_sim_max = -1\n",
    "        v4_loc = 0\n",
    "            \n",
    "        # find the closest word vector (answer) to the acquired vector in terms of cosine similarity\n",
    "        for i in range(len(M2_300_np)):\n",
    "            cos_sim = cosine_similarity(X=M2_300_np[i].reshape(1, -1), Y=v4_expected.reshape(1, -1))\n",
    "            \n",
    "            # if the word is more similar to the expected v4\n",
    "            # than any other previously seen words, record that word\n",
    "            # * the words in the analogy question is ignored\n",
    "            if cos_sim_max < cos_sim and M2_300.index[i] != w1 and M2_300.index[i] != w2 and M2_300.index[i] != w3:\n",
    "                cos_sim_max = cos_sim\n",
    "                v4_loc = i\n",
    "\n",
    "        answer = M2_300.index[v4_loc]\n",
    "        \n",
    "        if answer == w4:\n",
    "            syn_correct_count += 1\n",
    "            syn_correct_list.append([question, answer])\n",
    "        else:\n",
    "            syn_incorrect_list.append([question, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2vec accuracy on the semantic analogy test: 0.46, 251 out of 162\n",
      "Word2vec accuracy on the syntactic analogy test: 0.77, 1281 out of 2045\n",
      "LSA(300d) accuracy on the semantic analogy test: 0.17, 28 out of 162\n",
      "LSA(300d) accuracy on the syntactic analogy test: 0.053, 109 out of 2045\n"
     ]
    }
   ],
   "source": [
    "# Report the result\n",
    "\n",
    "print(f\"Word2vec accuracy on the semantic analogy test: {sem_result[0]:.2}, {len(sem_result[1][-1]['correct'])} out of {num_sem_test_cases}\")\n",
    "print(f\"Word2vec accuracy on the syntactic analogy test: {syn_result[0]:.2}, {len(syn_result[1][-1]['correct'])} out of {num_syn_test_cases}\")\n",
    "print(f\"LSA(300d) accuracy on the semantic analogy test: {sem_correct_count/num_sem_test_cases:.2}, {sem_correct_count} out of {num_sem_test_cases}\")\n",
    "print(f\"LSA(300d) accuracy on the syntactic analogy test: {syn_correct_count/num_syn_test_cases:.2}, {syn_correct_count} out of {num_syn_test_cases}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
